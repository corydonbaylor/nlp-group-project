<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Team Hanley NLP Working Group" />


<title>Exploring Text Data</title>

<script src="exploring-data_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="exploring-data_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="exploring-data_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="exploring-data_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="exploring-data_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="exploring-data_files/navigation-1.1/tabsets.js"></script>
<link href="exploring-data_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="exploring-data_files/highlightjs-9.12.0/highlight.js"></script>
<script src="exploring-data_files/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Exploring Text Data</h1>
<h4 class="author">Team Hanley NLP Working Group</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#summary-statistics-with-words">Summary Statistics with Words</a></li>
<li><a href="#word-clouds-with-wordcloud">Word Clouds with wordcloud</a></li>
<li><a href="#tf-idf">TF-IDF</a></li>
</ul>
</div>

<div id="summary-statistics-with-words" class="section level2">
<h2>Summary Statistics with Words</h2>
<p>The first step in any data science project should always be to get a sense of your data. For NLP, a lot of exploratory data analysis revolves around counting the frequencies of different terms and plotting them in different ways. This can be as simple as a bar chart looking at the count of distinct words, to word clouds, to something as complex as a TF-IDF.</p>
<p>Before we go any further, we first need to set up our workspace. We will primarily be working with <code>tidytext</code> to keep ourselves in the <code>tidyverse</code> and will be pulling data from <code>gutenbergr</code>, which is a repository of free classic texts. Let’s start by loading in our data, Metamorphosis by Kafka, and unnesting it into tokens.</p>
<pre class="r"><code>library(gutenbergr)
library(wordcloud)
library(ggplot2)
library(tidytext)
library(dplyr)
library(kableExtra)

meta = gutenberg_download(&quot;5200&quot;) 

tokens = meta %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words) </code></pre>
<p>Next we can create a simple bar plot looking at the top ten words (not including stop words of course) used in the book.</p>
<pre class="r"><code>word_count = tokens%&gt;%
  group_by(word)%&gt;%
  summarise(count = n())%&gt;%
  arrange(desc(count))%&gt;%
  slice(1:10)

ggplot(data = word_count)+
  geom_bar(aes(x = word, y = count), stat = &quot;identity&quot;,  fill = &quot;#6699cc&quot;)+
  theme_classic()</code></pre>
<p><img src="exploring-data_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="word-clouds-with-wordcloud" class="section level2">
<h2>Word Clouds with wordcloud</h2>
<p>Another way to present summary statistics is with a word cloud. For the uninitiated, a word cloud is just a collection of words with the size of the word determined by its prevalence. The natural disadvantage of this is that longer words will appear larger than smaller words, thus making them look more prevalent.</p>
<p>Actually creating the wordcloud is very easy. Just wrap a character vector in <code>wordcloud()</code>. We can remove some of the less frequent words using either the <code>min.freq</code> argument or the <code>max.words</code> argument.</p>
<pre class="r"><code>wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, &quot;Dark2&quot;))</code></pre>
<p><img src="exploring-data_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="tf-idf" class="section level2">
<h2>TF-IDF</h2>
<p>So far, we have looked at simply counting words and displaying this information in interesting ways. Next we will try to contextualize this information by observing how frequent words are in a document in comparison to other words.</p>
<p>TF-IDF, or Term Frequency and Inverse Document Frequency, is a way to numerically give importance to a word or phrase in a given text document relative to a collection of text documents.</p>
<p>In this example, we will examine how important a word is in a chapter of Metamorphosis in comparison to the entire novel. Our dataframe ‘meta’ contains the text from Metamorphosis; however, there is no column designating chapters in the novel. By looking through the ‘meta’ dataframe, we can see that there are chapter divides at rows 640 and 1296. Let’s start by adding a chapter column to our dataframe, dividing Metamorphosis into its three chapters.</p>
<pre class="r"><code>meta$chapter = NA
meta$chapter[1:639] = 1
meta$chapter[640:1295] = 2
meta$chapter[1296:nrow(meta)] = 3

meta &lt;- meta[,2:3]</code></pre>
<div id="term-frequency-tf" class="section level3">
<h3>Term Frequency (tf)</h3>
<p>First, we need to create a dataframe that breaks down our text into one word per row using <code>unnest_tokens()</code>. Here, we can count how many times words occur in each chapter. This is our term frequency (tf).</p>
<pre class="r"><code>book_words &lt;- meta%&gt;%
  unnest_tokens(word, text) %&gt;%
  group_by(chapter, word)%&gt;%
  summarise(n = n())%&gt;%
  arrange(desc(n))

book_words%&gt;%
  slice(1:2)%&gt;%
  kable()%&gt;%
  kable_styling(&quot;striped&quot;)</code></pre>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
chapter
</th>
<th style="text-align:left;">
word
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
the
</td>
<td style="text-align:right;">
386
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
to
</td>
<td style="text-align:right;">
251
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
the
</td>
<td style="text-align:right;">
382
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
to
</td>
<td style="text-align:right;">
254
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
the
</td>
<td style="text-align:right;">
380
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
and
</td>
<td style="text-align:right;">
253
</td>
</tr>
</tbody>
</table>
<p>Term frequency alone can tell us which words or phrases occur the most in a given document or collection of documents. This is helpful to some extent; however, some of the words or phrases with the highest term frequency may not be that important, or rather they may not give us much insight into what the document or collection of documents is about. In this example, we can see that “the” is the most frequent term in each chapter, giving us no insight into the contents of these chapters.</p>
<p>To fully illustrate this, let’s see how common words are across the entire book compared to any given chapter.</p>
<pre class="r"><code>meta%&gt;%
  unnest_tokens(word, text) %&gt;%
  group_by(word)%&gt;%
  summarise(book_count = n())%&gt;%
  arrange(desc(book_count))%&gt;%
  right_join(book_words, by = &quot;word&quot;)%&gt;%
  select(word, chapter, chapter_count = n, book_count)%&gt;%
  ungroup()%&gt;%
  slice(1:10)%&gt;%
  kable()%&gt;%
  kable_styling(&quot;striped&quot;)</code></pre>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
word
</th>
<th style="text-align:right;">
chapter
</th>
<th style="text-align:right;">
chapter_count
</th>
<th style="text-align:right;">
book_count
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
the
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
386
</td>
<td style="text-align:right;">
1148
</td>
</tr>
<tr>
<td style="text-align:left;">
the
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
382
</td>
<td style="text-align:right;">
1148
</td>
</tr>
<tr>
<td style="text-align:left;">
the
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
380
</td>
<td style="text-align:right;">
1148
</td>
</tr>
<tr>
<td style="text-align:left;">
to
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
254
</td>
<td style="text-align:right;">
753
</td>
</tr>
<tr>
<td style="text-align:left;">
and
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
253
</td>
<td style="text-align:right;">
642
</td>
</tr>
<tr>
<td style="text-align:left;">
to
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
251
</td>
<td style="text-align:right;">
753
</td>
</tr>
<tr>
<td style="text-align:left;">
to
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
248
</td>
<td style="text-align:right;">
753
</td>
</tr>
<tr>
<td style="text-align:left;">
he
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
230
</td>
<td style="text-align:right;">
577
</td>
</tr>
<tr>
<td style="text-align:left;">
and
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
206
</td>
<td style="text-align:right;">
642
</td>
</tr>
<tr>
<td style="text-align:left;">
his
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
206
</td>
<td style="text-align:right;">
550
</td>
</tr>
</tbody>
</table>
</div>
<div id="inverse-document-frequency-idf" class="section level3">
<h3>Inverse Document Frequency (idf)</h3>
<p>Where the term frequncy shows how common a word is the inverse document frequncy discounts words for being common across documents.</p>
<p>Mathematically, the IDF (inverse document frequency) of a word in a collection of documents can be understood as:</p>
<p>idf(word) = ln(total number of documents / number of documents containing word)</p>
<p>It may have been a minute since you took a math class, so let’s take a step back and think about what that natural log (ln) is doing there. The <a href="https://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/">natural log (ln)</a> can be thought of as the amount of time it takes something to grow exponentially from one. So ln(1) will be 0, since it takes no time to get to where you currently are.</p>
<p>In other words, if a word appears in all three documents it will have an IDF of 0. The more common the word across documents, the more it is discounted. This is in fact a good method to find context-specific stop words in a collection of documents. Words that appear frequently in every document (and have an IDF score of 0) may be good candidates for stop words.</p>
</div>
<div id="tf-idf-1" class="section level3">
<h3>TF-IDF</h3>
<p>Because you multiply the term frequency and the inverse document frequency together, in practice, this means that the TF <strong>boosts</strong> common words within a document, and the IDF <strong>discounts</strong> words that are common across documents.</p>
<p>Of course in R we can do all of this in one function. The <code>bind_tf_idf()</code> function gets the TF, IDF, and TF-IDF scores for each word in our dataset.</p>
<pre class="r"><code>book_words.2 &lt;- book_words %&gt;%
  bind_tf_idf(word, chapter, n)

book_words.2 %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
  group_by(chapter) %&gt;% 
  slice(1:10) %&gt;% 
  ungroup() %&gt;%
  ggplot(aes(word, tf_idf, fill = chapter)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  facet_wrap(~chapter, ncol = 2, scales = &quot;free&quot;) +
  coord_flip()+
  theme_classic()</code></pre>
<p><img src="exploring-data_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Looking at the words with the highest TF-IDF scores in each chapter, we can see which words are more important to individual chapters. Gregor Samsa is one of the main characters in this novel. The term “Gregor” occurred too frequently throughout the entire book. However, his last name, “Samsa,” was probably not used as often since we can see it has the highest TF-IDF score for chapter 3.</p>
<p>From this, we could make some guesses as to what the chapters in this book are about. Perhaps milk was spilled on the couch and someone needed money to buy new furniture in chapter 2.</p>
<p>The only way to know for sure would be to read the book, but this is of course more time efficient.</p>
<p>(You should still read the book – it’s quite good!)</p>
<p><br> <br></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
