---
title: "Introduction to Sentiment Analysis"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(kableExtra)
```

## Sentiment Analysis

Sentiment analysis is the classification and analysis of the emotional intent in text data. Some approaches include classifying words as positive or negative, assigning words to a specific emotion (like surprise), or calculating a sentiment score in the form of a number. 

This branch of NLP is often applied in consumer feedback and social media, and like all forms of data science is best used when the volume of data cannot be manually reviewed. If a company receives 50,000 pieces of user feedback, there needs to be a time efficient way of classifying them as positive or negative, and sentiment analysis is often used to fill this niche.

In this lesson, we will be using the `tidytext` package to perform sentiment analysis. This package allows us to work within the `tidyverse` and is compatible with other tidy packages such as `tidyr` and `dplyr`.

### Lexicons

`tidytext` has several sentiment lexicons that help in such analysis. To start, we'll take a look at "afinn", "bing", and "nrc". These lexicons are typically developed over multiple years of academic research and are meant to apply an as objective as possible classification of the typical emotion of a word.   

"afinn" assigns a number score, with higher numbers being more positive. You'll notice that "afinn" doesn't just classify something as positive or negative but also assigns a score based on how positive or negative that word is. 

```{r warning=FALSE, message=FALSE}

library(tidytext)

get_sentiments("afinn")%>%
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```

"bing" classifies words as either positive or negative. It does have considerably more words classified compared to "afinn" (6,786 v 2,477).

```{r warning=FALSE, message=FALSE}

get_sentiments("bing")%>%
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```

And "nrc" assigns words as either "yes" or "no" under categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust, meaning that any given word can have multiple sentiments attached to it.

```{r warning=FALSE, message=FALSE}

get_sentiments("nrc")%>%
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```

In the `tidytext` universe of sentiment analysis, the basic workflow for sentiment analysis will be to (1) remove stop words, (2) join on sentiments, and (3) prepare for presentation. Let's walk through a few examples of this. 

## Sentiment Analysis on Tweets

Let's look at some Tom Cruise tweets and see how we can apply sentiment analysis through wordclouds. We'll start by pulling in our tweets and cleaning up our text. You can find a tutorial on pulling tweets [here](https://nlp-working-group.netlify.app/nlp/twitter-tutorial.html). 

```{r warning=FALSE, message=FALSE}


library(wordcloud)
library(RColorBrewer)
library(rtweet)
source("../keys.R")

twitter_token <- create_token(
  app = twitter_app,
  consumer_key = twitter_api_key,
  consumer_secret = twitter_api_secret_key,
  access_token = twitter_access_token,
  access_secret = twitter_access_token_secret
)

tomcruise = get_timeline("@TomCruise", n =500)

text = tomcruise %>% select(text)%>%
  mutate(text = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", tomcruise$text),
         linenumber = row_number())



```

Now we'll make a wordcloud for positive words in these tweets, as assigned using the "bing" sentiment lexicon. First, we will use `unnest_tokens()` to tokenize the dataset (break each tweet into one word in each row). Then we will perform an `anti_join()` on the `stop_words` dataset provided by `tidytext` to remove common, mostly meaningless words. 

Finally, we will perform a `left_join` on the "bing" dictionary to return a classification for each remaining word. 

```{r warning=FALSE, message=FALSE}


sentiment = text%>% #this allows us to retain the row number/the tweet
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("bing"))
```

Once we have the a classified dataset, we will filter down to just the positive words and then plot them in a wordcloud. 

```{r warning=FALSE, message=FALSE}

positive_sentiment = sentiment%>% filter(!is.na(sentiment),
         sentiment == 'positive') # gets sentiment score based on bing dictionary

wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))


```

And the negative words...

```{r warning=FALSE, message=FALSE}


negative_sentiment = sentiment%>% filter(!is.na(sentiment),
         sentiment == 'negative') # gets sentiment score based on bing dictionary

wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))


```

As you can see, Tom Cruise is a pretty positive guy. He pretty much only tweets positive words. In fact, a few of the most prominent words in his negative word cloud are from the names of his *Mission Impossible* movies. 

## Sentiment Radar Chart

Sentiment is obviously not just positive or negative. The human experience (and Tom Cruise's tweets) are full of a myriad of experiences.

A radar chart is the perfect plot to represent the multi-polarity of the the "nrc" lexicon. The `fmsb` package allows us to quickly create a radar chart. 

```{r warning=FALSE, message=FALSE}

library(fmsb)
library(tidyr)

nrc_sentiment = text%>% #this allows us to retain the row number/the tweet
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("nrc"))%>%
  filter(!is.na(sentiment))


```

The `radarchart()` function accepts a dataframe where each column is a pole in the radar chart. Additionally, it requires a row representing the minimum and a row representing the maxiumum of the chart.  

```{r warning=FALSE, message=FALSE}

# spread so that each column is a sentiment
nrc_sentiment = nrc_sentiment%>%
  group_by(sentiment)%>%
  summarise(count = n())%>%
  spread(sentiment, count)

# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)

radarchart(nrc_sentiment, axistype=1 , 
    pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 , 
    cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
    vlcex=0.8 )


```

## Changing Sentiment in Metamorphosis

Let's finish by returning to the "bing" lexicon and make a chart representing the change in sentiments throughout the course of Franz Kafka's Metamorphosis. In our final visualization, we will create a bar chart where the x axis represents 25 line chunks of text, and the y axis represents the overall sentiment of that chunk. 

Let's start by tokenizing the text. Before tokenizing, let's add a line number. We will use this later to "chunk" the book into sections of 25 lines of text.

```{r warning=FALSE, message=FALSE}

library(gutenbergr)
library(tidyr)
library(ggplot2)

meta = gutenberg_download("5200")

book_words <- meta%>%
  mutate(linenumber = row_number())%>%
  unnest_tokens(word, text)
```

Now that the book is tokenized, we need to first get sentiments for each word and then create an index in order to bin them into bars. 

We will use the `%/%` operator to perform this binning. This operator performs interger division, rounding down to the nearest whole number. The first 24 lines of text will be in the 1st bin, the next 24 lines will be in the next bin and so on. Each bin will represent a single bar on our barchart. 

```{r warning=FALSE, message=FALSE}

sentbars = book_words %>%
  inner_join(get_sentiments("bing"))%>%
  # %/% performs interger divison, rounding down to the nearest whole number
  mutate(index = linenumber %/% 25)%>% 
  group_by(index, sentiment)%>%
  summarise(count = n())%>%
  spread(sentiment, count, fill = 0) %>%
  mutate(sentiment = positive - negative,
         sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
  ungroup()

sentbars%>%
  slice(1:3)%>%
  kable()%>%
  kable_styling("striped")

```

Now that we have prepared our data, we need to plot it. We need a pretty good label for the x axis as this is a non-standard plot. We should also remove the x axis line, ticks, and text because what they represent will likely be confusing to the reader. 

```{r warning=FALSE, message=FALSE}

ggplot(data = sentbars) +
  geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
  theme_classic()+
  theme(
    legend.position = "none",
    axis.ticks.x = element_blank(),
    axis.line.x = element_blank(),
    axis.text.x = element_blank()
  )+
  scale_fill_manual(values = c("darkred", "darkgreen"))+
  labs(title = "The Metamorphosis of Sentiment",
       x = "Change over Time (Each Bar is 25 Lines of Text)",
       y = "Sentiment Score")


```

## Valence Shifters

How would the above strategies work with a sentence like "I am not happy". Well if we are assigning a sentiment on the word level, we will get a positive score for the word "happy" and as a result, the sentence will be rated as positive, even when this is clearly not the case. 

Enter valence shifters. 

While this term may sound like its out of Startrek, the concept is actually quite simple. Certain words like "not" or "very" serve to modify different words. In your 4th grade english class, these would be called adverbs or adjectives, but in data science, we call these valence shifters. 

Some valence shifters are negators (no, not), some are amplifiers (very, extremely), and some are deamplifiers (hardly, rarely).

### Sentimentr

The `sentimentr` package allows us to handle these complex sentence structures. This package scores sentiment by looking at the totality of the observation rather than individual words, so you will not need to tokenize by word.

There are two main function in `sentimentr`: `sentiment()` and `sentiment_by()`. Let's see how they work:

```{r}
library(sentimentr)

sentimentr::sentiment("I am barely happy. In fact, I am very sad")%>%
  kable()%>%
  kable_styling("striped")

sentimentr::sentiment_by("I am barely happy. In fact, I am very sad")%>%
  kable()%>%
  kable_styling("striped")

```

`sentiment()` takes a text string, breaks it up into sentences, and then scores those sentences. Whereas, `sentiment_by()` scores the text string as a whole by averaging the two sentence's scores together. 

Because `sentiment_by()` has a one to one relationship with its input, it can potentially be used in a tidy framework, like so:

```{r}

sample = data.frame(
  text = c(
    "I am so happy",
    "I am not very happy",
    "I am unhappy",
    "I very unhappy",
    "I am sad",
    "I am lots of things, happy, unhappy, I contain multitudes"
    ), 
  linenumber = seq(1:6)
)

sample%>%
  mutate(sentimentr_score = sentiment_by(text)$ave_sentiment)%>%
  kable()%>%
  kable_styling("striped")

```

Now that we understand how this function operates, lets compare it to our previous strategy of using the "afinn" dictionary and the `tidytext` methodology:

```{r}

sample%>%
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("afinn"))%>% # uses afinn to dictionary for scores
  group_by(linenumber)%>% # takes it back up to the line number level
  summarise(afinn_score = mean(value, na.rm = T))%>% # summarises by score
  right_join(sample)%>% # joins back to sample to get the full sentence
  # adds sentiment scoring via sentimentr
  mutate(sentimentr_score = sentiment_by(text)$ave_sentiment)%>%
  select(text, afinn_score, sentimentr_score)%>%
  kable()%>%
  kable_styling("striped")

```

Comparing the two, we can see that `sentimentr` was able to catch the negation in "I am not very happy" and the distinction between "I am unhappy" and "I am very unhappy" (as a sidenote, I am perfectly happy but am not creative enough for better example sentences). So considering this, why would we ever use `tidytext`?

In general, the `tidytext` method is much easier to understand and communicate. Do not underestimate how important it is for a client to understand how your model works and why a given observation was labled a particular way. While `sentimentr` relies on a [complex algorithm](https://cran.r-project.org/web/packages/sentimentr/readme/README.html) to assign a score. When choosing between `tidytext` and `sentimentr`, it will often boil down to the task, and if a easy to explain model, with some inherent limitations, is sufficient.   