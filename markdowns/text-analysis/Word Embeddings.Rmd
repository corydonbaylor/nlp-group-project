---
title: "How to create a GloVe (Machine learning) Model in R
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    self_contained: false
---


#What are word embeddings?

Have you ever wanted to compare apples and oranges? How similar are they really? One common measure of similarity in mathematics is distance. For example, you could say that the number 4 is more similar to the number 5 than 20 because it is *closer* to 5 than 20. 

But whats the distance between the word apple and the word orange?

Enter word embeddings.

In its simplest terms, Word Embeddings are representing words with numbers with the implicit goal of having similar words be represented by similar numbers. 

More specifically, words are represented by vectors. Let's take the previous sentence and convert it into a character vector

```{r}

words = c("more", "specifically", "words", "are", "represented", "by", "vectors")

```

Now we got a character vector. If we wanted to represent the word "more" in this character vector mathematically, we could do the following:

```{r}

more = c(1, 0, 0, 0, 0, 0, 0)

```

The first element in our "more" vector is 1 because the first element in our words vector is "more". 

This is just a taste of how one might represent words and sentences as vectors and is meant to give you a baseline/mental image as we add complexity.

In this lesson, we will be using pretrained models for word embeddings. Popular algorithms categorized under word embedding include Embedding Layer, Word2Vec, and GloVe. For this section, we will be training and implementing a GloVe algorithm. 

## So what is Glove?

The Global Vectors for Word Representation (GloVe) algorithm is an unsupervised learning model used to obtain vector representations for words. GloVe maps words in such a way that the distance between words corresponds to semantic similarity. 

The word vectors created using the GloVe algorithm can estimate the mathematical relationship between words based on how often the words appear in a similar context. The word vector spaces produced can be added and subtracted from one another. 

For example, theoretically if you take the vector for "king" and subtract "Man" and "add Women" you should get close to the vector space value of "Queen".

More information about GloVe can be found here --> https://nlp.stanford.edu/projects/glove/

## Data Prep for GloVe

Let's start by loading the "text2vec" library to get access to the GloVe model. Additionally, we are going to be a working with a large dataset of over 200,000 highly compressed Wikipedia articles called text8. text8 was originally compiled and hosted at http://mattmahoney.net/dc/textdata.

Put the text8.zip in the same folder as you are working in order to follow along (double check your working directory if you cant get it loaded in).

```{r}

library(text2vec)

text8_file = "~/text8"

unzip ("~/text8.zip", files = "text8", exdir = "~/")

wiki = readLines(text8_file, n = 1, warn = FALSE)

```

Now that we have our data, we can create tokens using the "space_tokenizer()" function. After creating tokens we will need to create itoken using the "itoken()" functions and then create a vocabulary using the "create_vocabulary()" function. 


```{r}
# Create iterator over tokens
tokens <- space_tokenizer(wiki) 

# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

After we have created our vocabulary, we have to eliminate the uncommon words. We can do this by using the "prune_vocabulary" function. We will reduce words that appear less than five times.

Once the vocabulary is cleaned, we need to vectorize the words. This can be done using the "vocab_vectorizer" and feeding in your cleaned vocabulary as a parameter. 

```{r}
vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)
```

Following the vectorization of the words, you will need to create TCM (term-co-occurrence matrix). This can be done using the "create_tcm" function. 

```{r}
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
```

After Creating the TCM we are finally ready to create and train our GloVe model.  

## Using the GloVe Model

First you will need to initiate the model using the "GlobalVectors$new" function.

Then you can fit transform the model using the "glove$fit_transform" function. 
Please be aware that -- depending on the parameters that are chosen for the "glove$fit_transform" -- this will be very time- and memory-intensive. Depening on your computer, this can take over 30 minutes.

```{r}
glove = GlobalVectors$new(rank = 50, x_max = 10)
shakes_wv_main = glove$fit_transform(tcm, n_iter = 100, convergence_tol = 0.001)
```

With the model trained and fitted we now need to create the individual word vectors. 

```{r}
wv_context = glove$components
word_vectors = shakes_wv_main  + t(wv_context)
```

With all the individual words having been converted into vectored space we can now mathematically measure the similarity between words. 

We know that "Paris" is the capital of "France" and "Berlin" is the capital of "Germany". Therefore the difference between "Paris" and "France" should be similar to "Germany" and "Berlin" in terms of vectorized space. So if we take the vector space of "Paris" and subtract the vector space of France and add the vector space of Germany we should get close to the value of the vector space of Berlin. 


```{r}
berlin = word_vectors["paris", , drop = FALSE] -
  word_vectors["france", , drop = FALSE] +
  word_vectors["germany", , drop = FALSE]
```

If we want to find the closest values to the vector space of the Berlin equation we created above, we will need to use cosine similarity. 

```{r}
berlin_cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
head(sort(berlin_cos_sim[,1], decreasing = TRUE), 5)
```



