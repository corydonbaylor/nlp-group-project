---
title: "Preparing Text Data"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text Normalization

Text data pre-processing, or normalization, is the process of converting text data into a more machine-readable form before analyzing it. This normalization entails a number of processes, which can include tokenization, stop words removal, and text normalization, most commonly in the form of stemming and lemmatization. 

That might sound like a lot, but no worries! This tutorial will dive into each of these approaches for preparing text data.

## Tokenizing by n-grams

A computer cannot read a novel. At least not in the way that you or me do. A computer needs text to be broken down into something more digestible in order to work with it. **Tokenization** is the process by which big chunks of text are broken down into smaller more digestible chunks, oftentimes into single words. 

In this tutorial, we will cover tokenization and perform some basic data cleaning that will transform a long string of text into discrete oberservations that R can work with more easily. 

### Getting the Data

But first, we need data! We will be using the book Metamorphosis by Franz Kafka as our example data. 

```{r warning=FALSE, message=FALSE}

library(dplyr)
library(tidytext)
library(gutenbergr)
library(kableExtra)
library(tidyr)

```

### Unigrams

An n-gram is a contiguous sequence of n words from a sample of text. A unigram is a single word, a bigram is two words, a trigram is three words, and so on and so on. 

Tokenizing by unigrams is easy! We'll use our text from Metamorphosis.

```{r, message=F, warning=F, error = F}

# downloading metamorphosis
meta = gutenberg_download("5200")

unigrams = meta %>% unnest_tokens(word, text, token = "ngrams", n = 1)

unigrams%>%
    # below is purely for visuzliations purposes
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```

### Bigrams

And here's an example of tokenizing by bigrams. Notice the only argument in `unnest_tokens` that needs to change from our unigram example is `n =`. (The first argument listed here is the column name, which can be whatever you want.)

```{r, message=F, warning=F, error = F}


bigrams = meta %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

# below is purely for visuzliations purposes
bigrams%>%
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```


## Removing Stop Words
Stop words are words that are typically more important to the grammar of a sentence rather than the meaning of a sentence. They are like helper words that help string together meaning without providing much meaning themselves. 

For humans, more words are needed, but for NLP they add nothing. Let's look at that last sentence for example.

"For humans, more words are needed but for NLP they add nothing."

Now without the stop words!

"humans more words needed NLP add nothing."

As you can see, computer really only need cave man speak to get the semantic meaning of a string of text. But now that we know that computer's don't necessarily need all the words in a string of text, how do we know what words to use?

As you might guess, what exactly constitutes a stop word is somewhat subjective. However, researchers have put together lists of words that typically do not contain much semantic meaning. We are going to be using a list (also called a lexicon in this context) to filter out the stop words. This list comes with the `tidytext` package and is loaded in when you load the library. It can be found by typing `View(stop_words)` into the console. 

So then, we have a list of words that we will filter out contained in the `stop_words` dataset. If we are working with unigrams than the job is easy. We just need to filter out unigrams, so they do not contain any stop words, like so:

```{r warning = FALSE, message = FALSE}

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)

```


## Stemming and Lemmatization

After tokenization and stop word removal, you can further prepare your text data by breaking your words down to their more fundamental meaning-bearing units through text normalization. To understand this, it's helpful to take a brief look at some relevant terminology.

Closely tied to text normalization is **morphology**, the study of the structure of words -- particularly, of the smaller parts of words that have meaning, called **morphemes**. Simple words don't have internal structure and only consist of one morpheme (e.g., "eat", "weak", "build"). Complex words consist of two or more morphemes (e.g., "worker", "quicker"). In the case of "worker", "er" is added to the root word "work" to create a noun. Morphological normalization is the process of breaking down words to their root forms. 

Also important to understand is inflection. Inflection is the modification of words to express different grammatical categories. For example, for the root word "walk", we can add the inflection -ed to indicate that the walking occurred in the past or -ing to indicate that the walking is ongoing. 

The two most common ways of text normalization -- breaking words down to their core parts -- are stemming and lemmatization.

### Stemming

**Stemming** is the process of reducing inflection in words to their "root" forms through the use of an algorithm. Put simply, an algorithm looks for patterns that typically indicate inflection like "ed" or "ing" and removes it in order to reduce the word to it's stem or root.

While there are a number of stemming algorithms, we'll take a look at the results of the Snowball stemming algorithm, which comes built in to the R library `SnowballC`.

```{r, warning=FALSE, message=F}

library(SnowballC)

wordStem(c("love", "loving", "lovingly", "loved", "lover", "lovely", "love"))

```

As we can see, the algorithm was able to break down most of these words to the root "love". Let's take a look at another example.

```{r, warning=FALSE, message=F}

wordStem(c("fish", "fisher", "fishing", "fished", "fishery"))

```


Our results are starting to look fishy, so to speak. While "fish", "fishing", and "fished" broke down to "fish" (or stayed put in the case of "fish"), "fisher" didn't change and "fishery" became "fisheri", not something in the English language last time I checked.

Herein lies an inherent limitation of stemming. While this approach is relatively  computationally efficient and more inclusive in terms of identifying similar words, it sometimes trims words down inappropriately, namely with overstemming or understemming.

**Overstemming** is when too much of a word is cut off, resulting either in nonsensical stems, where the word has lost its meaning (e.g., "fisheri" above), or in words being resolved to the same stems when they shouldn't be. For example, "university", "universal", "universities", and "universe" could resolve to the stem "univers", instead of having "universal" and "universe" resolve to "univers", while "university" and "universities" resolve to "universi". 

**Understemming** is when two words should be stemmed to the same root but are not. A common example is the words "alumnus", "alumni", and "alumnae" resolving to "alumnu", "alumni", and "alumna".

Stemming in tidytext is quite easy. Let's go ahead and stem the unigrams that we pulled earlier. Combine the `mutate()` function with the `wordStem()` function to overwrite the word column with the newly stemmed values. 

```{r, warning=FALSE, message=F}

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)%>%
  mutate(word = wordStem(word))

unigrams_cleaned%>%
  slice(10:15)%>%
  kable()%>%
  kable_styling("striped")

```

### Lemmatization

The other most common text normalization approach is lemmatization. According to this helpful [article](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), lemmatization "usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma." That is to say, lemmatization ensures that the root word belongs to the language. 

Unlike stemming, which uses an algorithm to trim down to a root word, lemmatization refers to the WordNet corpus to produce a root word, much like looking up a word in a dictionary.

For example, a lemmatizer should map "gone", "going", and "went" into "go".

There are a few ways we can lemmatize in R. First we'll check out an example using the library `textstem`. 

```{r, warning=FALSE, message=F}

library(textstem)
lemmatize_words(c("eat", "ate", "eaten", "eating", "eaten"))

```

As you can see the root word for our entire vector of root words seems much more reasonable now! Lets repeat the example from stemming with lemmatization. The only thing we need to change is the `wordStem()` function to `lemmatize_words()`. 

```{r, warning=FALSE, message=F}

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)%>%
  mutate(word = lemmatize_words(word))

unigrams_cleaned%>%
  slice(10:15)%>%
  kable()%>%
  kable_styling("striped")

```

Notice that thus far the words under examination are in a vector. If we want to perform lemmatization on a corpus, we'll need to use the `tm` library.

```{r, warning=FALSE, message=F}

library(tm)

example = c('Jog', 'jogging', 'jogged', 'ran', 'run', 'running', 'am', 'is', 'are', 'was',
            'were', 'be', 'being', 'been') 
df = as.data.frame(example) 
corpus = Corpus(VectorSource(df$example))  
corpus <- tm_map(corpus, lemmatize_strings)
inspect(corpus)

```

## Cleaning n-grams

So far, when removing stop words and performing stemming and lemmatization, we have been working with unigrams instead of bigrams or trigrams. The strategies thus far only work with unigrams because all the methods require a vector of single words to work, rather than a vector of multiple words. 

For example, you cannot filter out the word "the" from the text string "the dog" because "the" does not equal "the dog". How can we get around this? 

The simplest method is to perform all your cleaning on unigrams, recreate the original text, and then unnest the tokens into however many n-grams you need. 

Let's walk through an example. In our example, we are going to have two text strings that we need to clean and then eventually tokenize into bigrams.

```{r, warning=FALSE, message=F}

df = data.frame(
  example_num = c(1, 2),
  text = c("Here are two examples. I wrote them down for you. In each example you will find a lot of stop words", 
                    "Another thing you might find is inflected words or words that could be stemmed")
  )


```

We are going to remove the stop words and lemmatize next. 

```{r, warning=FALSE, message=F}

clean = df%>%
  unnest_tokens(word, text, token = "ngrams", n = 1)%>%
  filter(!word %in% stop_words$word)%>% # removing stop words
  mutate(word = lemmatize_words(word))

```

Finally, using `group_by` to retain the example number and `summarise` plus `paste` to collapse the text back into a single line, we will recreate the orginal text, but now cleaned. At this point, we can unnest the lemmatized and cleaned words into however many ngrams that we want without having to leave the tidytext format. 

```{r, warning=FALSE, message=F}

back_again = clean%>%
  group_by(example_num)%>%
  summarise(text = paste(word, collapse = " "))

df2 = back_again %>% unnest_tokens(word, text, token = "ngrams", n = 2)

```

However, there is a bit of an asterisk here. If you remove stop words in this way, once you unnest the tokens, you will be creating trigrams and bigrams from the *cleaned* words rather than the original words, which aren't techincally consecutive. 

If this is a problem for your analysis. [Follow tidytext's guide to removing stop words](https://www.tidytextmining.com/ngrams.html#counting-and-filtering-n-grams). Using this method for lemmatization or stemming should not be a problem. 

## Example: Trigrams for Heart of Darkness {.tabset .tabset-fade}

Overall, this tutorial is simply meant to acquaint you with the basic premises of preparing text data for analysis, specifically in the tidytext universe. 

Now, try to repeat the above steps for "Heart of Darkness" by Joseph Conrad. But this time, let's use trigrams instead of bigrams. **The Gutenberg id is 219**.

### Problem

We will be organizng the book "Heart of Darkness" into **trigrams**. You will need all the packages and skill sets described above to prepare "Heart of Darkness" for analysis. 

**Hints**
- `tidytext` is mostly scalable but will require some minor edits to handle trigrams
- Make sure you remove stop words from *all three* of the trigrams.

### Solution

**Step 1: Download the Book**
```{r, warning = F, message=F}

darkness = gutenberg_download("219")

```

**Step 2: Tokenize into Unigrams and Clean**
```{r, warning = F, message=F}

darkness %>%
  unnest_tokens(word, text, token = "ngrams", n = 1)%>%
  filter(!word %in% stop_words$word)%>% # removing stop words
  mutate(word = lemmatize_words(word))%>%
  # just for visualization
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```

**Step 3: Group Back into the Text**
```{r, warning = F, message=F}

darkness %>%
  unnest_tokens(word, text, token = "ngrams", n = 1)%>%
  filter(!word %in% stop_words$word)%>% # removing stop words
  mutate(word = lemmatize_words(word))%>% 
  group_by(gutenberg_id)%>%
  summarise(text = paste(word, collapse = " "))%>%
  # below is purely for visuzliations purposes
  mutate(text = substr(text, 1, 50))%>% # first 50 characters otherwise we will display the whole book
  kable()%>%
  kable_styling("striped")

```

**Step 4: Unnest into Trigrams**
```{r, warning = F, message=F}

darkness %>%
  unnest_tokens(word, text, token = "ngrams", n = 1)%>%
  filter(!word %in% stop_words$word)%>% # removing stop words
  mutate(word = lemmatize_words(word))%>% 
  group_by(gutenberg_id)%>%
  summarise(text = paste(word, collapse = " "))%>%
  unnest_tokens(word, text, token = "ngrams", n = 3)%>%
  # below is purely for visuzliations purposes
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")

```
