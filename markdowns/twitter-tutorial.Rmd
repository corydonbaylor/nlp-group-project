---
title: "Twitter Data with rtweet"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
---

```{r, include=F}
library(rtweet)
source("../keys.R")

twitter_token <- create_token(
  app = twitter_app,
  consumer_key = twitter_api_key,
  consumer_secret = twitter_api_secret_key,
  access_token = twitter_access_token,
  access_secret = twitter_access_token_secret
)


```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Getting Started

### Getting a Developer Account
[How to create a Twitter Developer Account](https://www.extly.com/docs/autotweetng_joocial/tutorials/how-to-auto-post-from-joomla-to-twitter/apply-for-a-twitter-developer-account/#apply-for-a-developer-account)

### Saving Credentials in a .gitignore
After finally getting your credentials, you need to protect them. If you are just working locally on your computer, this is easy. Just do whatever you usually do to protect complicated passwords. However, if you are working on a git project (like this one), then you need to do something extra.

Luckily, the process is very simple. (1) Save your credentials in a different script, (2) add that script into your .gitignore, and then (3) add that script into the .gitignore. 

**Save your credentials into a script called keys.R**
```{}
# twitter
twitter_api_key = "sample_key"
twitter_api_secret_key = "sample_key"
twitter_access_token = "sample_key"
twitter_access_token_secret = "sample_key"

```

**Create a .gitignore file**

This part is a little more complicated than you may expect because you are just creating a file. But I think the most reproducible way is through git bash. In the root directory of your project, open up bash (right click and press "Git Bash Here"), and write the following code:

`touch .gitignore`

**Add script into your .gitignore and then call it into your project**
Just open up the .gitignore (Notepad works fine), and add keys.R into the .gitignore file. Now if you are simply just writing `keys.R` into the file, then it has to be in the same directory. There are a bunch of different options for [.gitignore](https://www.atlassian.com/git/tutorials/saving-changes/gitignore). Finally, make sure that you call the keys script into your project so that the keys are in your environment. 

## rtweet

`rtweet` is a [package](rtweet.info) designed to interact with Twitter's API. It is tidyverse ready meaning that its API calls return a data.frame. This is really convienient if your workflow includes things like `dplyr` or `tidytext`.

### Retrieving Topics
In order to retrieve topics from Twitter with the rtweet package, you must create a Twitter Developer account to further create your very own unique API access tokens (keys). Follow the instructions on this [link](https://rtweet.info/articles/auth.html) to do so.
```{r, message=F, warning=F, error = F}
library(tokenizers)
library(dplyr)
library(tidytext)
library(tidyr)
library(rtweet)
library(ggplot2)

# Get the top 50 trending hashtags in the US. 
usa <- get_trends("United States")
usa$trend

# Let's search for 1000 tweets hashtagging the number one trending hashtag in the US
top_trending_hashtag <- search_tweets(
  usa$trend[1], n = 1000, include_rts = FALSE) %>%
  select(screen_name, text, hashtags)

head(top_trending_hashtag) 

# Removing URLs and numbers attached to words
top_trending_hashtag$text <- gsub("http.*", "", top_trending_hashtag$text)
top_trending_hashtag$text <- gsub("https.*", "", top_trending_hashtag$text)
top_trending_hashtag$text <- gsub("[0-9]","",top_trending_hashtag$text)

# Creating a table in a format that can later be broken down by n words per row
text_df <- tibble(user = top_trending_hashtag$screen_name, text = top_trending_hashtag$text)

head(text_df)

# Create a table of all 2 word phrases (bigrams) in tweets hashtagged with the top trending hashtag in the US
bigram_table <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigram_table

# Separate the bigrams into individual word columns in order to remove stopwords
# Stopwords are common words such as 'a' or 'and' that do not provide meaning in text analysis
bigram_table <- bigram_table %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Filter out stopwords
bigrams_filtered <- bigram_table %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)


# Combine the individual word columns back into one bigram column
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Create a count column that counts the number of times each bigram appears within our data set
bigrams_united <- bigrams_united %>%
  count(bigram, sort = TRUE)


bigrams_united %>%
  top_n(15) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(x = bigram, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique Phrases",
       title = "Count of unique phrases found in tweets",
       subtitle = "Stop words removed from the list")



```


### User Data
caitlin

### Posting a tweet
Corydon





### Saving Credentials in a .gitignore


## TwitteR

TwitteR is a package providing access to Twitter's API, "with a bias towards API calls that are more useful in data analysis as opposed to daily interaction," according to [rdocumentation.org](https://www.rdocumentation.org/packages/twitteR/versions/1.1.9). It supports a number of functions, including retrieving trends and tweets with users' handles.  

### Retrieving Topics

To retrieve a trending topic, you can start by going to Twitter to see what matters to the world: 

![](images/caturday.PNG)

It appears people like cats for some reason. No matter -- let's see if we can retrieve the last 500 tweets with the trending "#Caturday" hashtag.

First, call the twitteR package and run the `setup_twitter_oauth` function, which will set up your credentials for the twitteR session.

```{r, message=F, warning=F}
library(twitteR)
setup_twitter_oauth(twitter_api_key, 
                    twitter_api_secret_key,
                    twitter_access_token, 
                    twitter_access_token_secret)
```

Notice the keys and tokens in the `setup_twitter_oauth` function above. I've already read these keys into variables saved in the global environment. 

Next, create an object to store the string you want to search, set the number of tweets you wish to pull, and run the `searchTwitter` function. Notice you can set the "lang" argument to search for tweets in specific languages. I set lang equal to "en" for tweets in English.  

```{r, message=F, warning=F}
catweets <- searchTwitter('#Caturday', n = 10, lang='en')
```

Finally, put the retrieved tweets into a dataframe.

```{r, message=F, warning=F}
df <- do.call("rbind", lapply(catweets, as.data.frame))

library(kableExtra)

kable(df[1:2,1:2], format = "html")%>%
  kable_styling("striped")
```

### User Tweets

You can also retrieve tweets containing a user's handle. Let's see what the esteemed Gordon Ramsay is up to.

![](images/giphy.gif)

First, let's run the `searchTwitter` function for 100 tweets with the handle "@GordonRamsay".

```{r, message=F, warning=F}
tweets_gordon <- searchTwitter('@GordonRamsay', n = 100)
```

Then, let's get the text from those tweets.

```{r, message=F, warning=F}
feed_gordon = plyr::laply(tweets_gordon, function(t) t$getText())

kable(head(feed_gordon), format = "html")%>%
  kable_styling("striped")
```

Ta-da.


## Example 1: Pulling Data for a User {.tabset .tabset-fade} 

### Problem

Create a scatter plot showing the retweet to favorites ratio for Dwayne "the Rock" Johnson. 

**Hints**

- Using the `rtweet` and the `get_timeline()` function you can pull back all the data (and waay more) that you need. 
- `ggplot2` can allow you to easily put together a simple scatterplot

### Solution

Due to the simplicity of the `rtweet` package, retrieving the data isn't all that invovled. Simply use the `get_timeline()` function and boom, you got the data.

```{r, message=F, warning=F}

rock = get_timeline("therock", n =100)
```

Now we just need to put it into a chart. I like the `theme_minimal()` function from ggplot for a simple and professional look.  

```{r, message=F, warning=F}
library(scales)
library(ggplot2)

ggplot(data = rock)+
  geom_point(aes(x = favorite_count, # how many favorited 
                 y = retweet_count,  # how many retweeted
                 color = is_quote    # is the rock quoting?
                 )
             )+ 
  theme_minimal()+
  theme(legend.position = "bottom")+
  # adding the right title and legend label
  labs(title = "What's the Rock Cooking?", 
       color = "Is the Rock Quoting?")+
  # using comma from the scales package for the axis labels
  scale_y_continuous("Retweets", labels = comma)+ 
  scale_x_continuous("Favorites", labels = comma)

```

Well it looks like the Rock wasn't cooking up to much as of 3/18/2020. We got one really popular tweet and that's it.

## Example 2: Search a Topic on Twitter {.tabset .tabset-fade}

### Problem
Now let us enter the terrifying world of regex. Specifically when it comes to cleaning up tweets. 

For our next example, lets pull all the **popular** tweets using with the hashtag #rstats and then clean out all the hashtags and carriage returns that come with it.

**Hint**
Personally, I think this tests your googling ability much more than than your mastery of the package. I used the following [stackoverflow](https://stackoverflow.com/questions/51947268/remove-hashtags-from-beginning-and-end-of-tweets-in-r) as the basis of my response. 

### Solution
```{r, message=F, warning=F}
data = search_tweets("#rstats", # include the topic you are searching for 
                     type  = "popular", # lets return the 'popular' tweets
                     include_rts = F) # no retweets pls.
data$text
```

That looks...gross. Lets remove the hashtags and carriage returns!

```{r, message=F, warning=F}
text = gsub("(?:\\s*#\\w+)*", "", data$text) # remove all the hashtags at the end
text = gsub("\n|â€”", "", text) # remove all the line breaks
text = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", text) # remove urls

text
```

Ah, much better!

## Conclusion
