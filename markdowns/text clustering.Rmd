---
title: "Introduction to Text Clustering"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
---

## Text Clustering

Clustering is the organization of unlabelled objects such that objects in the same group are similar to each other and dissimilar to those in other groups. Text clustering is such a process applied to text data.

As someone smart points out [here](https://cai.tools.sap/blog/introduction-text-clustering/), text clustering has three requirements: a "distance measure to define whether or not two documents are similar, a criterion function to compute the quality of our clusters and finally an algorithm to optimize this criterion."

While there are several common forms of clustering, this tutorial -- which draws on a very helpful vignette [here](https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html) -- will demonstrate how to do document clustering using hierarchical clustering. 

Like other types of cluster analysis, hierarchical clustering uses an algorithm to group similar objects into clusters. It can be performed either on a distance matrix or raw data. When we just have raw data, we must compute a distance matrix. The output of our hierarchical clustering will be a dendogram. You can find more about how hierarchical clustering works [here](https://www.displayr.com/what-is-hierarchical-clustering/#:~:text=Hierarchical%20clustering%2C%20also%20known%20as,broadly%20similar%20to%20each%20other.).

For our example, we'll examine a dataset that contains the texts of 200 news articles from 2015. We'll begin by reading in the csv.


```{r warning=FALSE, message=FALSE}

library(data.table)
library(textmineR)
library(stringr)
library(dplyr) 
setwd("C:/Users/603281/Desktop/NLP_R_Practice/nlp_github_repo/nlp-group-project/")
# source("../")

df = fread("articles_clean.csv")

# text = df %>% select(Article)%>%
#   mutate(Article = gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", df$Article))

```


Then we'll make a document term matrix and create a matrix of term counts to get the inverse document frequency (IDF) vector.


```{r warning=FALSE, message=FALSE}

# create a document term matrix 
dtm <- CreateDtm(doc_vec = df$Article, # character vector of documents
                 # doc_names = df$cord_uid, # document names
                 ngram_window = c(1, 2), # minimum and maximum n-gram length
                 stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
                                  stopwords::stopwords(source = "smart")), # this is the default value
                 lower = TRUE, # lowercase - this is the default value
                 remove_punctuation = TRUE, # punctuation - this is the default
                 remove_numbers = TRUE, # numbers - this is the default
                 verbose = FALSE, # Turn off status bar for this demo
                 cpus = 2
                 ) # default is all available cpus on the system

# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)

```




```{r warning=FALSE, message=FALSE}

# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf

tfidf <- t(tfidf)

csim <- tfidf / sqrt(rowSums(tfidf * tfidf))

csim <- csim %*% t(csim)



cdist <- as.dist(1 - csim)

hc <- hclust(cdist, "ward.D")

clustering <- cutree(hc, 10)

plot(hc, main = "Hierarchical clustering of 200 news articles",
     ylab = "", xlab = "", yaxt = "n")

rect.hclust(hc, 10, border = "red")

```



```{r warning=FALSE, message=FALSE}

library(kableExtra)

p_words <- colSums(dtm) / sum(dtm)

cluster_words <- lapply(unique(clustering), function(x){
  rows <- dtm[ clustering == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows <- rows[ , colSums(rows) > 0 ]
  
  colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})


# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
                              size = as.numeric(table(clustering)),
                              top_words = sapply(cluster_words, function(d){
                                paste(
                                  names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                  collapse = ", ")
                              }),
                              stringsAsFactors = FALSE)

kable(cluster_summary, format = "html")%>%
  kable_styling("striped")

```



```{r warning=FALSE, message=FALSE}

# plot a word cloud of one cluster as an example
wordcloud::wordcloud(words = names(cluster_words[[ 5 ]]), 
                     freq = cluster_words[[ 5 ]], 
                     max.words = 50, 
                     random.order = FALSE, 
                     colors = c("red", "yellow", "blue"),
                     main = "Top words")

```

