---
title: "Exploring Text Data"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

We will be using the book Metamorphosis by Franz Kafka as our example data. 

```{r warning=FALSE, message=FALSE}

library(dplyr)
library(tidytext)
library(gutenbergr)

meta = gutenberg_download("5200")

```

## Word Clouds with wordcloud

This section will show how to make wordclouds from different text data. We'll also show how to use sentiment analysis to make wordclouds made up of positive and negative words.

We'll start by analyzing tweets from Tom Cruise's Twitter account.

First, we'll call the packages we need and source our Twitter API keys.


```{r warning=FALSE, message=FALSE}
library(rtweet)
library(tm)
library(wordcloud)
library(plyr)
library(RSentiment)
source("../keys.R")

```


Next, we'll activate our API keys, pull in the last 500 tweets from Mr. Cruise's timeline, and set the text column from the resulting dataframe to the object "text".


```{r warning=FALSE, message=FALSE}

twitter_token <- create_token(
  app = twitter_app,
  consumer_key = twitter_api_key,
  consumer_secret = twitter_api_secret_key,
  access_token = twitter_access_token,
  access_secret = twitter_access_token_secret
)

tomcruise = get_timeline("@TomCruise", n =500)

text <- as.character(tomcruise$text)

```


Next, we'll clean up our data a bit. We'll start by making the text data into a corpus and then removing unnecessary elements, such as punctuation and white space. 


```{r warning=FALSE, message=FALSE}

corpus <- Corpus(VectorSource(list(text)))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))


```


Cool. Let's check out what we have so far.


```{r warning=FALSE, message=FALSE}

wordcloud(corpus)

```


That's a lot to take in. Let's make a positive and negative wordcloud using the package RSentiment. We'll start by making a document term matrix, then add a term frequency column to it, then calculate the sentiment for our words.


```{r warning=FALSE, message=FALSE, echo=T, results='hide'}

dtm_up <- DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_up <- colSums(as.matrix(dtm_up))

sentiments_up <- calculate_sentiment(names(freq_up))
sentiments_up <- cbind(sentiments_up, as.data.frame(freq_up))
sent_pos_up <- sentiments_up[sentiments_up$sentiment == 'Positive',]
sent_neg_up <- sentiments_up[sentiments_up$sentiment == 'Negative',]
cat("Negative sentiments: ", sum(sent_neg_up$freq_up), "Positive sentiments :", sum(sent_pos_up$freq_up))

```


Let's see what our positive word cloud looks like.


```{r warning=FALSE, message=FALSE}

wordcloud(sent_pos_up$text, sent_pos_up$freq_up, min.freq = 1, random.order = FALSE, colors=brewer.pal(6, "Dark2"))

```


And our negative word cloud.


```{r warning=FALSE, message=FALSE}

wordcloud(sent_neg_up$text, sent_neg_up$freq_up, min.freq = 1, random.order = FALSE, colors=brewer.pal(6, "Dark2"))

```


Looks like wordclouds aren't quite the Mission Impossible. I'll see myself out...

Just kidding -- we have one more example. Let's run the same code to make positive and negative wordclouds for our Metamorphosis text. 


```{r warning=FALSE, message=FALSE, echo=T, results='hide'}

text <- as.character(meta$text)

corpus <- Corpus(VectorSource(list(text)))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm_up <- DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
freq_up <- colSums(as.matrix(dtm_up))

sentiments_up <- calculate_sentiment(names(freq_up))
sentiments_up <- cbind(sentiments_up, as.data.frame(freq_up))
sent_pos_up <- sentiments_up[sentiments_up$sentiment == 'Positive',]
sent_neg_up <- sentiments_up[sentiments_up$sentiment == 'Negative',]
cat("Negative sentiments: ", sum(sent_neg_up$freq_up), "Positive sentiments :", sum(sent_pos_up$freq_up))

```


Here's a positive wordcloud:


```{r warning=FALSE, message=FALSE}

wordcloud(sent_pos_up$text, sent_pos_up$freq_up, min.freq = 3, random.order = FALSE, colors=brewer.pal(6, "Dark2"))


```


And the negative one:


```{r warning=FALSE, message=FALSE}

wordcloud(sent_neg_up$text, sent_neg_up$freq_up, min.freq = 3, random.order = FALSE, colors=brewer.pal(6, "Dark2"))

```


And there you have it.

## TF-IDF

TF-IDF, or Term Frequency and Inverse Document Frequency is a way of to numerically give importance to a word or phrase in a given text document relative to a collection of text documents. 


In this example, we will examine how important a word is in a chapter of Metamorphosis in comparison to the entire novel. Our dataframe 'meta' contains the text from Metamorphosis, however there is no column designating chapters in the novel. By looking through the meta dataframe, we can see that there are chapter divides at rows 640 and 1296. Let's start by adding a chapter column to our dataframe, dividing Metamorphosis into it's 3 chapters.
```{r warning=FALSE, message=FALSE}
meta$chapter[1:639] <- 1
meta$chapter[640:1295] <- 2
meta$chapter[1296:nrow(meta)] <- 3

meta.2 <- meta[,2:3]
```
Next, we need to create a dataframe that breaks down our text into one word per row using unnest_tokens(). Here, we can count how many times words occur in each chapter. This is our term frequency (tf).
```{r warning=FALSE, message=FALSE}
book_words <- meta.2 %>%
  unnest_tokens(word, text) %>%
  add_count(chapter, word, sort = TRUE)

book_words <- distinct(book_words)

book_words %>%
  arrange(desc(n))
```
Term frequency alone can tell us which words or phrases occur the most in a given document or collection of documents. This is helpful to some extent, however some of the words or phrases with the highest term frequency may not be that important, or rather they may not give us much insight into what the document or collection of documents is about. In this example, we can see that "the" is the most frequent term in each chapter,giving us no insight into the contents of these chapters. 

The IDF (inverse document frequency) of a word in a collection of documents can be understood as:

idf(word) = ln(total number of documents/number of documents containing word)

(ln is the natural log)

In this example, the number of documents is 3 since we have 3 total chapters. A term with an idf score of 0 is a term that occurs very frequently across the entire collection of text. The word "the" occurs in all 3 chapters. 

IDF example: 

idf("the") = ln(3/3) = 0

The bind_tf_idf() function gets the tf, idf, and tf-idf scores for each word in our dataset.
```{r warning=FALSE, message=FALSE}
book_words.2 <- book_words %>%
  bind_tf_idf(word,chapter, n)

book_words.2

book_words.2 %>%
  arrange(desc(tf_idf)) 

book_words.2 %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(chapter) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = chapter)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~chapter, ncol = 2, scales = "free") +
  coord_flip()


```
Looking at the words with the highest tf-idf scores in each chapter, we can see which words are more important to individual chapters. Gregor Samsa is one of the main characters in this novel. The term "Gregor" occurred too frequenty throughout the entire book, however his last name, "Samsa" was probably not used as often since we can see it has the highest tf-idf score for chapter 3.

From this, we could make some guesses as to what the chapters in this book are about. Perhaps milk was spilled on the couch and somone needed money to buy new furniture in chapter 2.


## Unique Word Counts
Sometimes, we may want to see how many words occur in a document or set of documents. We may also want to know how many times that word occurs in a document or set of documets. We will go through a brief example of how to do that now.

First, we will create a dataframe that has a column counting the total number of times each word occurs across the entire novel.
```{r warning=FALSE, message=FALSE}
library(data.table)
total_word_counts <- meta.2 %>%
  unnest_tokens(word, text) %>%
  add_count(word)
```
Next, we use our dataframe, 'book_words' from the tf-idf example, which has a column counting the number of times each word occurs in a chapter.
```{r warning=FALSE, message=FALSE}
book_words <- meta.2 %>%
  unnest_tokens(word, text) %>%
  add_count(chapter, word, sort = TRUE)

book_words <- distinct(book_words)
```
Joining the two dataframes together and changing column names appropriately:
```{r warning=FALSE, message=FALSE}
word_counts <- book_words %>%
  left_join(total_word_counts, by = c("word","chapter"))

word_counts <- distinct(word_counts)


setnames(word_counts, old = "n.x", new = "chapter_count")
setnames(word_counts, old = "n.y", new = "book_count")

word_counts %>%
  arrange(desc(chapter_count))
```
Now we can see unique word counts by chapter and across the entire book.

As you can see, there are many ways to give importance to words in text data. Word/phrase counts (term frequency) and tf-idf are two great places to start when analytzing text data.
