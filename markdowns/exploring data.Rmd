---
title: "Exploring Text Data"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary Statistics with Words

The first step in any data science project should always be to get a sense of your data. For NLP, a lot of exploratory data analysis revolves around counting the frequencies of different terms and plotting them in different ways. This can be as simple as a bar chart looking at the count of distinct words, to word clouds, to something as complex as a TF-IDF. 

Before we go any further, we first need to set up our workspace. We will primarily be working with `tidytext` to keep ourselves in the `tidyverse` and will be pulling data from `gutenbergr`, which is a repository of free classic texts. Let's start by loading in our data, Metamorphosis by Kafka, and unnesting it into tokens.

```{r warning=FALSE, message=FALSE}
library(gutenbergr)
library(wordcloud)
library(ggplot2)
library(tidytext)
library(dplyr)
library(kableExtra)

meta = gutenberg_download("5200") 

tokens = meta %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) 

```

Next we can create a simple bar plot looking at the top ten words (not including stop words of course) used in the book. 

```{r warning=FALSE, message=FALSE}

word_count = tokens%>%
  group_by(word)%>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)

ggplot(data = word_count)+
  geom_bar(aes(x = word, y = count), stat = "identity",  fill = "#6699cc")+
  theme_classic()

```

## Word Clouds with wordcloud

Another way to present summary statistics is with a word cloud. For the uninitiated, a word cloud is just a collection of words with the size of the word determined by its prevalence. The natural disadvantage of this is that longer words will appear larger than smaller words, thus making them look more prevalent.  

Actually creating the wordcloud is very easy. Just wrap a character vector in `wordcloud()`. We can remove some of the less frequent words using either the `min.freq` argument or the `max.words` argument. 

```{r warning=FALSE, message=FALSE}

wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, "Dark2"))

```

<br>
<br>
