---
title: "Tokening with tidytext"
author: "Team Hanley NLP Working Group"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Getting the Data

We will be using the book Metamorphosis by Franz Kafka as our example data. 

```{r warning=FALSE, message=FALSE}
library(gutenbergr)
meta = gutenberg_download("5200")

```
## Tokening by n-grams
Michael

### Unigrams
Michael

### Bigrams
Michael

## Removing Stop Words
Stop words are words that are typically more important to the grammer of a sentence rather than the meaning of a sentence. They are like helper words that help string together meaning without providing much meaning themselves. 

For humans, more words are needed but for NLP they add nothing. Let's look at that last word for example.

>For humans, more words are needed but for NLP they add nothing.

Now without the stop words!

>humans more words needed NLP add nothing.

As you can see, computer really only need cave man speak to get the semantic meaning of words.

### How do we know if something is a stop word?
As you might guess, what exactly constitutes a stop word is somewhat subjective. When thinking about defining a stop word, we need to think about generalities and scale. For example, "no" can be a really important word semantically, and for certain tasks, like sentiment analysis, [you may want to keep certain stop words](https://medium.com/@limavallantin/why-is-removing-stop-words-not-always-a-good-idea-c8d35bd77214), especially words like "no" or "not". Handling these words is called [negation handling](https://towardsdatascience.com/sentiment-analysis-in-r-good-vs-not-good-handling-negations-2404ec9ff2ae). But we will get into that when we cover sentiment analysis.   

Lets use our bigram example from before. We are going to be basing our methodogly for removing stop words off of [tidytextmining](https://www.tidytextmining.com/ngrams.html#counting-and-filtering-n-grams). Its pretty simple and somewhat scalable.

```{r, warning=FALSE, message=F}
library(tidytext)
library(dplyr)
library(tidyr)
library(kableExtra)

unnest = meta%>%
  unnest_tokens(bigram, text, token = "ngrams", n =2)
```


- First split the ngrams into their own columns. In this case, since its bigrams, the columns will be "word1" and "word2"
- Next for each "word" column, filter out all the stop words
- Finally, concatinate all the columns back together with `unite()`

```{r, warning=FALSE, message=F}

final = unnest%>%
  separate(bigram, c("word1", "word2"), sep = " ")%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  unite(bigram, word1, word2, sep = " ")

# preparing for display
final%>%
  slice(1:4)%>%
  kable()%>%
  kable_styling("striped")



```
## Counting n-grams
While it may seem simple, counting frequency of n-grams (in our case bigrams) will tell us a lot about the content of the text. Lets look at the bigram frequency of the *Metamorphasis* with an without removing stop words:

**With Stop Words**
```{r, warning=FALSE, message=F}
unnest%>%
  count(bigram, sort = T)%>%
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")
```

**Without Stop Words**
```{r, warning=FALSE, message=F}
final%>%
  count(bigram, sort = T)%>%
  slice(1:5)%>%
  kable()%>%
  kable_styling("striped")
```


So as we can see, before we remove the stop words, we largely get unremarkable bigrams at the top of the list. In fact, I would guess that you would "of the" as one of the top bigrams in nearly any book or paper. 

Once we remove the stop words, we begin to generate content that is specific to our book. Interestingly, "earn money" is one of the top five bigrams in the book!

## TF-IDF
Caitlin

## Example 1:

Caitlin

## Example 2:

Caitlin